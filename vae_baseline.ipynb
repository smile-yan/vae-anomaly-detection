{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae-baseline",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHM46dUu3GS4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "from scipy.stats import multivariate_normal\r\n",
        "\r\n",
        "\r\n",
        "class VAE(tf.keras.Model):\r\n",
        "    \"\"\"Class of basic Variational Autoencoder\r\n",
        "\r\n",
        "    This class contains basic components and main functions of VAE model.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "      latent_dim: size of latent variables. 2,4,6 ... 2n\r\n",
        "      input_shape: shape of input data. [28,28]...    \r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, latent_dim,input_shape):\r\n",
        "        super(VAE, self).__init__()\r\n",
        "        self.latent_dim = latent_dim\r\n",
        "        \r\n",
        "        # Encoder NN\r\n",
        "        self.encoder = tf.keras.Sequential(\r\n",
        "          [\r\n",
        "            tf.keras.layers.InputLayer(input_shape=input_shape),\r\n",
        "            tf.keras.layers.Flatten(),\r\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\r\n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim),\r\n",
        "          ]\r\n",
        "        )\r\n",
        "\r\n",
        "        # Decoder NN\r\n",
        "        self.decoder = tf.keras.Sequential(\r\n",
        "          [\r\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim)),\r\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\r\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "            tf.keras.layers.Dense(784),\r\n",
        "            tf.keras.layers.Reshape(target_shape=input_shape),\r\n",
        "          ]\r\n",
        "        )\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def sample(self, eps=None):\r\n",
        "        \"\"\" sample data from latent variables \r\n",
        "\r\n",
        "        Args:\r\n",
        "          eps: it will be used for decode and\r\n",
        "            eps will be set by a random data when None\r\n",
        "        Return:\r\n",
        "          log x'(the reconstruction data of x)\r\n",
        "        \"\"\"\r\n",
        "        if eps is None:\r\n",
        "            eps = tf.random.normal(shape=(100, self.latent_dim))\r\n",
        "        return self.decode(eps, apply_sigmoid=True)\r\n",
        "\r\n",
        "    def encode(self, x):\r\n",
        "        \"\"\"do encode jobs\r\n",
        "        for each input data x, will construct multiple Gausian Distribution\r\n",
        "\r\n",
        "        Args:\r\n",
        "          x: data that will be encoded.\r\n",
        "        Return:\r\n",
        "          mean: array like. list of mu of that distribution.\r\n",
        "          logvar: array like. list of logvar of that distribution.        \r\n",
        "        \"\"\"\r\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\r\n",
        "        return mean, logvar\r\n",
        "\r\n",
        "    def reparameterize(self, mean, logvar):\r\n",
        "        \"\"\" reparameterize trick \"\"\"\r\n",
        "        eps = tf.random.normal(shape=mean.shape)\r\n",
        "        return eps * tf.exp(logvar * .5) + mean\r\n",
        "    \r\n",
        "    # decode\r\n",
        "    def decode(self, z, apply_sigmoid=False):\r\n",
        "        \"\"\" do decode job\r\n",
        "        Args:\r\n",
        "          z: latent variables.\r\n",
        "          apply_sigmoid: use sigmoid or not.\r\n",
        "        Return:\r\n",
        "          log x'(the reconstruction data of x)\r\n",
        "        \"\"\"\r\n",
        "        logits = self.decoder(z)\r\n",
        "        if apply_sigmoid:\r\n",
        "            probs = tf.sigmoid(logits)\r\n",
        "            return probs\r\n",
        "        return logits\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\r\n",
        "    \"\"\"Log of the normal probability density function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      sample: sample that need compute pdf\r\n",
        "      mean,logvar: parametes of distribution\r\n",
        "    Return:\r\n",
        "      log pdf \r\n",
        "    \"\"\"\r\n",
        "    log2pi = tf.math.log(2. * np.pi)\r\n",
        "    return tf.reduce_sum(\r\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\r\n",
        "      axis=raxis)\r\n",
        "\r\n",
        "def compute_loss(model, x):\r\n",
        "    \"\"\"calc ELBO\r\n",
        "    \r\n",
        "    Args: \r\n",
        "      x: input data\r\n",
        "    Return:\r\n",
        "      loss\r\n",
        "    \"\"\"\r\n",
        "    # 1. encode process \r\n",
        "    mean, logvar = model.encode(x)\r\n",
        "\r\n",
        "    # 2. reparameterize \r\n",
        "    z = model.reparameterize(mean, logvar)\r\n",
        "    \r\n",
        "    # 3. get log x'\r\n",
        "    x_logit = model.decode(z)\r\n",
        "    \r\n",
        "    # 4. Compute sigmoid cross entropy given `logits`. \r\n",
        "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\r\n",
        "    \r\n",
        "    # 5. Compute log p(x|z),log p(z),log q(z|x)\r\n",
        "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\r\n",
        "    logpz = log_normal_pdf(z, 0., 0.)\r\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\r\n",
        "\r\n",
        "    # 5. ELBO \r\n",
        "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\r\n",
        "\r\n",
        "# optimizers\r\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(model, x, optimizer):\r\n",
        "    \"\"\"Executes one training step and returns the loss.\r\n",
        "\r\n",
        "    This function computes the loss and gradients, and uses the latter to\r\n",
        "    update the model's parameters.\r\n",
        "    \"\"\"\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        loss = compute_loss(model, x)\r\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "def reconstructed_probability(X):\r\n",
        "    \"\"\"Get the reconstruction probability for X\r\n",
        "\r\n",
        "    Args:\r\n",
        "      X: test data. X.shape = (32,28,28,1)\r\n",
        "    Return:\r\n",
        "      log of probability   \r\n",
        "    \"\"\"\r\n",
        "    reconstructed_prob = np.zeros(X.shape[0], dtype='float32')\r\n",
        "\r\n",
        "    # 1. encode, \r\n",
        "    mu_hat, log_sigma_hat = model.encode(X)\r\n",
        "    sigma_hat = tf.exp(log_sigma_hat)+0.0001\r\n",
        "    \r\n",
        "    # for each piece of data\r\n",
        "    for j in range(X.shape[0]):\r\n",
        "        p_l = multivariate_normal.logpdf(X[j], mu_hat[j,:], np.diag(sigma_hat[j,:]))\r\n",
        "        reconstructed_prob[j] += tf.reduce_sum(p_l)\r\n",
        "    \r\n",
        "    return reconstructed_prob\r\n",
        "\r\n",
        "def judege_anomaly(scores, threshold):\r\n",
        "    \"\"\" judge anomaly\r\n",
        "    if scores[i] > threshold:\r\n",
        "        label for scores[i]: 1\r\n",
        "    Args:\r\n",
        "      scores: Array like.\r\n",
        "      threshold: float\r\n",
        "    Return:\r\n",
        "      labels: Array like.\r\n",
        "    \"\"\"\r\n",
        "    labels = np.zeros(len(scores),dtype=int)\r\n",
        "    for i in range(len(scores)):\r\n",
        "        if (scores[i]>threshold):\r\n",
        "            labels[i] = 1\r\n",
        "\r\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cqVzZbjyq_t",
        "outputId": "ccc0749d-373a-406f-b776-d6d051e32847"
      },
      "source": [
        "model = VAE(latent_dim=4,input_shape=(28,28,1))\r\n",
        "\r\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n",
        "\r\n",
        "# preprocess image data\r\n",
        "def preprocess_images(images):\r\n",
        "    images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\r\n",
        "    return np.where(images > .5, 1.0, 0.0).astype('float32')\r\n",
        "\r\n",
        "# exclude error num\r\n",
        "def exclude_num(x, y, num):\r\n",
        "    keep = (y != num) \r\n",
        "    x = x[keep]\r\n",
        "    return x \r\n",
        "\r\n",
        "# label anomaly\r\n",
        "def label_anomaly(anomaly_num,labels):\r\n",
        "    true_labels = np.zeros(len(labels),dtype=int)\r\n",
        "    for i in range(len(labels)):\r\n",
        "        if (labels[i] == anomaly_num):\r\n",
        "            true_labels[i] = 1\r\n",
        "    return true_labels        \r\n",
        "\r\n",
        "train_images = preprocess_images(train_images)\r\n",
        "test_images = preprocess_images(test_images)\r\n",
        "print(\"Before exclusion, Train_images.shape:\",train_images.shape)\r\n",
        "\r\n",
        "#choose error num\r\n",
        "anomaly_num = 8\r\n",
        "train_images = exclude_num(train_images,train_labels,anomaly_num)\r\n",
        "\r\n",
        "# get true labels\r\n",
        "true_labels = label_anomaly(anomaly_num, test_labels)\r\n",
        "\r\n",
        "print(\"Take num {} as error data.\".format(anomaly_num))\r\n",
        "print(\"After exclusion, Train_images.shape:\",train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before exclusion, Train_images.shape: (60000, 28, 28, 1)\n",
            "Take num 8 as error data.\n",
            "After exclusion, Train_images.shape: (54149, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdL1-hhbTBY0"
      },
      "source": [
        "# get validataion data from train_images\r\n",
        "validation_size = int(train_images.shape[0]*0.3)\r\n",
        "train_data = train_images[:-validation_size]\r\n",
        "validation_data = train_images[-validation_size:]\r\n",
        "\r\n",
        "# test_data \r\n",
        "# test_dataset = test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "476TaEKiy4f0"
      },
      "source": [
        "train_size = 60000\r\n",
        "batch_size = 32\r\n",
        "test_size = 10000\r\n",
        "\r\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_data)\r\n",
        "                 .shuffle(train_size).batch(batch_size))\r\n",
        "\r\n",
        "validation_dataset = (tf.data.Dataset.from_tensor_slices(validation_data)\r\n",
        "                 .shuffle(train_size).batch(batch_size))\r\n",
        "\r\n",
        "# for test data, do not shuffle\r\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\r\n",
        "                .batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_61Z11d4y6-X",
        "outputId": "fd323d98-632b-4223-da85-4003bd30b28e"
      },
      "source": [
        "epochs = 20\r\n",
        "\r\n",
        "# training    \r\n",
        "for epoch in range(1, epochs + 1):\r\n",
        "    # training\r\n",
        "    start_time = time.time()\r\n",
        "    for train_x in train_dataset:\r\n",
        "        train_step(model, train_x, optimizer)\r\n",
        "    end_time = time.time()\r\n",
        "\r\n",
        "    # compute loss\r\n",
        "    loss = tf.keras.metrics.Mean()\r\n",
        "    for validation_x in validation_dataset:\r\n",
        "        loss(compute_loss(model, validation_x))\r\n",
        "    elbo = -loss.result()\r\n",
        "\r\n",
        "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\r\n",
        "        .format(epoch, elbo, end_time - start_time))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Test set ELBO: -200.57557678222656, time elapse for current epoch: 2.9963009357452393\n",
            "Epoch: 2, Test set ELBO: -177.3914794921875, time elapse for current epoch: 1.893111228942871\n",
            "Epoch: 3, Test set ELBO: -162.7436981201172, time elapse for current epoch: 1.8526606559753418\n",
            "Epoch: 4, Test set ELBO: -155.03526306152344, time elapse for current epoch: 1.885533332824707\n",
            "Epoch: 5, Test set ELBO: -149.99581909179688, time elapse for current epoch: 1.8758111000061035\n",
            "Epoch: 6, Test set ELBO: -146.64480590820312, time elapse for current epoch: 1.8865952491760254\n",
            "Epoch: 7, Test set ELBO: -144.0478057861328, time elapse for current epoch: 1.8395683765411377\n",
            "Epoch: 8, Test set ELBO: -141.7947540283203, time elapse for current epoch: 1.8848865032196045\n",
            "Epoch: 9, Test set ELBO: -140.01817321777344, time elapse for current epoch: 1.8557243347167969\n",
            "Epoch: 10, Test set ELBO: -138.4994659423828, time elapse for current epoch: 1.8587820529937744\n",
            "Epoch: 11, Test set ELBO: -137.1979522705078, time elapse for current epoch: 1.8500185012817383\n",
            "Epoch: 12, Test set ELBO: -136.19602966308594, time elapse for current epoch: 1.8651707172393799\n",
            "Epoch: 13, Test set ELBO: -135.19143676757812, time elapse for current epoch: 1.891812801361084\n",
            "Epoch: 14, Test set ELBO: -134.53305053710938, time elapse for current epoch: 1.884974718093872\n",
            "Epoch: 15, Test set ELBO: -133.70770263671875, time elapse for current epoch: 1.91282057762146\n",
            "Epoch: 16, Test set ELBO: -133.0389862060547, time elapse for current epoch: 1.8745267391204834\n",
            "Epoch: 17, Test set ELBO: -132.50828552246094, time elapse for current epoch: 1.8924403190612793\n",
            "Epoch: 18, Test set ELBO: -131.91281127929688, time elapse for current epoch: 1.8892037868499756\n",
            "Epoch: 19, Test set ELBO: -131.3972930908203, time elapse for current epoch: 1.8811023235321045\n",
            "Epoch: 20, Test set ELBO: -131.0538330078125, time elapse for current epoch: 1.882176399230957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFS24PjISgms"
      },
      "source": [
        "# def judege_anomaly(scores,threshold):\r\n",
        "#     labels = np.zeros(len(scores),dtype=int)\r\n",
        "#     for i in range(len(scores)):\r\n",
        "#         if (scores[i]>threshold):\r\n",
        "#             labels[i] = 1\r\n",
        "\r\n",
        "#     return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM4IEQ7bzQdq"
      },
      "source": [
        "# true_labels = np.zeros(len(test_labels),dtype=int)\r\n",
        "# for i in range(len(test_labels)):\r\n",
        "#     if (test_labels[i] == error_num):\r\n",
        "#         true_labels[i] = 1\r\n",
        "# true_labels        \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDzXFh6Vu2xc",
        "outputId": "20747e7f-afb0-4c89-e25d-028c8b8edee1"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from time import sleep\r\n",
        "\r\n",
        "predict = []\r\n",
        "with tqdm(total=len(test_dataset)) as pbar:\r\n",
        "    for test_data in test_dataset:\r\n",
        "        results = reconstructed_probability(X=test_data)\r\n",
        "        pre_labels = judege_anomaly(results,-31*1000)\r\n",
        "        predict.extend(pre_labels)\r\n",
        "        pbar.update(1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:18<00:00, 16.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACGLdbIf0vP6",
        "outputId": "fc57b8e7-e198-4ae2-d8bb-a07ee9fa814f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "accuracy_score(predict,true_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxzBXVCfxvKm",
        "outputId": "4a9240e0-948b-4cc3-8236-90da2bdde1be"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edQ2cN5w6ENG",
        "outputId": "3823d8bc-255e-4daf-863c-29f953bcd911"
      },
      "source": [
        "count/len(true_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}